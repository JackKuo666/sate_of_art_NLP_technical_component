# 1.词向量之加载word2vec和glove
https://blog.csdn.net/u010041824/article/details/70832295
用Glove预训练的词向量也可以用gensim加载进来，只是在加载之前要多做一步操作
# 2.关于交叉熵+softmax 与ppl
## 2.1 交叉熵+softmax
https://www.tinymind.cn/articles/1223
## 2.2 ppl
https://img-blog.csdn.net/20171224140203272
这里应该是以e为底
# 3.beam search 
原理：https://zhuanlan.zhihu.com/p/36029811?group_id=972420376412762112

解释：

1.用于Encoder-Decoder框架，用于测试阶段，

2.要生成样本了（即采样）的时候比如，对于样本（a，b）预测下一个值，但是值的可能性其实不止一种（可能是c，可能是d），

3.比如beamsize=2的时候选取最大的两个可能性（每个样本的概率，实际上经常是会通过softmax函数归一化取得的）




beam search 与viterbi 区别

1. beam search 的操作属于贪心算法思想，不一定reach到全局最优解。因为考虑到seq2seq的inference阶段的搜索空间过大而导致的搜索效率降低，所以即使是一个相对的局部优解在工程上也是可接受的。

2. viterbi属于动态规划思想，保证有最优解。viterbi应用到宽度较小的graph最优寻径是非常favorable的，毕竟，能reach到全局最优为何不用！

作者：Scofield
链接：https://www.zhihu.com/question/54356960/answer/324590737
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。
